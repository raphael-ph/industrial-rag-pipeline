{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "521e92c2",
   "metadata": {},
   "source": [
    "# Evaluating the Quality of a RAG Pipeline\n",
    "\n",
    "When developing **Retrieval-Augmented Generation (RAG)** pipelines, it is essential to ensure that the system performs reliably across two critical dimensions:\n",
    "\n",
    "1. **Retrieval Quality**\n",
    "\n",
    "   * The pipeline must accurately identify and retrieve all documents relevant to the user query.\n",
    "   * Retrieved documents should be **highly relevant**, minimizing noise and irrelevant information that could mislead the generation stage.\n",
    "   * Evaluation metrics may include recall, precision, and relevance scoring based on domain-specific criteria.\n",
    "\n",
    "2. **Generation Accuracy**\n",
    "\n",
    "   * The content generated by the RAG model must faithfully reflect the information contained in the retrieved documents.\n",
    "   * **No hallucinations** or unsupported statements should be present; the generated output should be verifiable against the source documents.\n",
    "   * Consider evaluating fluency, factual consistency, and adherence to context in addition to relevance.\n",
    "\n",
    "For evaluating the created RAG pipeline, we will use the provided samples at `tests/samples` and metrics inspired by the RAGAS framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbfc0cd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up the python path to use internal modules here\n",
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fff1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting env variables\n",
    "elastic_url = os.environ[\"ELASTIC_SEARCH_URL\"]\n",
    "elastic_api_key = os.environ[\"ELASTIC_SEARCH_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1119c971",
   "metadata": {},
   "source": [
    "## Creating the evaluation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d21360",
   "metadata": {},
   "source": [
    "### What is RAGAS?\n",
    "\n",
    "RAGAS (Retrieval Augmented Generation Assessment) provides metrics to evaluate RAG systems by measuring both retrieval quality and generation quality separately.\n",
    "\n",
    "### The 4 Core Metrics\n",
    "\n",
    "#### **Retrieval Evaluation**\n",
    "\n",
    "##### **Context Recall**\n",
    "**Measures:** Did we retrieve all the information needed to answer the question?\n",
    "\n",
    "- Compares ground truth answer with retrieved documents\n",
    "- Calculates: What percentage of required information was actually retrieved\n",
    "- **High score = Good:** Retrieved documents contain most/all relevant info\n",
    "- **Low score = Problem:** Missing key information in retrieval\n",
    "\n",
    "**Example:**\n",
    "- Question: \"What are Python's data types?\"\n",
    "- Ground truth mentions: strings, integers, lists, dictionaries\n",
    "- Retrieved docs only mention: strings, integers\n",
    "- Context Recall = 50% (missing lists, dictionaries)\n",
    "\n",
    "##### **Context Relevance** \n",
    "**Measures:** How much retrieved information is actually relevant?\n",
    "\n",
    "- Analyzes each retrieved document for relevance to the query\n",
    "- Calculates: Percentage of relevant content in retrieved documents\n",
    "- **High score = Good:** Retrieved docs are mostly on-topic\n",
    "- **Low score = Problem:** Too much irrelevant information retrieved\n",
    "\n",
    "**Example:**\n",
    "- Question: \"How to install Python?\"\n",
    "- Retrieved 10 sentences: 7 about Python installation, 3 about Java\n",
    "- Context Relevance = 70%\n",
    "\n",
    "#### **Generation Evaluation**\n",
    "\n",
    "##### **Faithfulness**\n",
    "**Measures:** Is the generated answer factually consistent with retrieved documents?\n",
    "\n",
    "- Breaks generated response into individual claims\n",
    "- Verifies each claim against the source documents\n",
    "- Calculates: Percentage of verifiable claims\n",
    "- **High score = Good:** Answer sticks to the facts from documents\n",
    "- **Low score = Problem:** Answer contains hallucinations or contradictions\n",
    "\n",
    "**Example:**\n",
    "- Retrieved doc: \"Python 3.9 was released in October 2020\"\n",
    "- Generated answer: \"Python 3.9 was released in October 2020 and supports async/await\"\n",
    "- First claim: verifiable ✓, Second claim: not in documents ✗\n",
    "- Faithfulness = 50%\n",
    "\n",
    "##### **Response Relevancy**\n",
    "**Measures:** How well does the answer address the original question?\n",
    "\n",
    "- Compares the generated response with the original question\n",
    "- Checks if response directly answers what was asked\n",
    "- **High score = Good:** Answer is complete and on-topic\n",
    "- **Low score = Problem:** Answer is vague, incomplete, or off-topic\n",
    "\n",
    "**Example:**\n",
    "- Question: \"What is machine learning?\"\n",
    "- Good answer: \"Machine learning is a subset of AI that enables computers to learn from data...\"\n",
    "- Poor answer: \"Computers are very useful tools in modern society...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2cef6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from dataclasses import dataclass, asdict\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Configure Google client (make sure your API key is set)\n",
    "google_client = genai.Client()\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    \"\"\"Data class to store evaluation results\"\"\"\n",
    "    question: str\n",
    "    ground_truth: str\n",
    "    retrieved_contexts: List[str]\n",
    "    generated_answer: str\n",
    "    document_recall: float\n",
    "    context_relevance: float\n",
    "    faithfulness: float\n",
    "    response_relevance: float\n",
    "    overall_score: float\n",
    "\n",
    "class RAGEvaluator:\n",
    "    \"\"\"\n",
    "    Comprehensive RAG evaluation system based on RAGAS metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model: str = \"gemini-2.5-flash\", \n",
    "                 max_retries: int = 10,\n",
    "                 delay_between_calls: float = 1.0):\n",
    "        self.model = model\n",
    "        self.max_retries = max_retries\n",
    "        self.delay_between_calls = delay_between_calls\n",
    "    \n",
    "    def _make_llm_call(self, prompt: str, temperature: float = 0.1) -> str:\n",
    "        \"\"\"Make an LLM call with retry logic and rate limiting\"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = google_client.models.generate_content(\n",
    "                    model=self.model,\n",
    "                    contents=[types.Content(\n",
    "                        role=\"user\",\n",
    "                        parts=[types.Part.from_text(text=prompt)]\n",
    "                    )],\n",
    "                    config=types.GenerateContentConfig(\n",
    "                        temperature=temperature,\n",
    "                        max_output_tokens=2048\n",
    "                    )\n",
    "                )\n",
    "                time.sleep(self.delay_between_calls)  # Rate limiting\n",
    "                return response.text.strip()\n",
    "            except Exception as e:\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    wait_time = (2 ** attempt) + random.uniform(0, 1)\n",
    "                    print(f\"API call failed (attempt {attempt + 1}/{self.max_retries}): {e}\")\n",
    "                    print(f\"Retrying in {wait_time:.2f} seconds...\")\n",
    "                    time.sleep(wait_time)\n",
    "                else:\n",
    "                    print(f\"Max retries reached. Final error: {e}\")\n",
    "                    raise\n",
    "    \n",
    "    def evaluate_document_recall(self, ground_truth: str, retrieved_contexts: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate how well the retrieved contexts cover the ground truth information.\n",
    "        Uses semantic similarity to assess coverage.\n",
    "        \"\"\"\n",
    "        if not retrieved_contexts:\n",
    "            return 0.0\n",
    "        \n",
    "        # Extract key facts from ground truth\n",
    "        fact_extraction_prompt = f\"\"\"\n",
    "        Extract the key facts and information from the following ground truth answer. \n",
    "        List each fact as a separate bullet point.\n",
    "        \n",
    "        Ground Truth: {ground_truth}\n",
    "        \n",
    "        Key Facts:\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            facts_response = self._make_llm_call(fact_extraction_prompt)\n",
    "            facts = [fact.strip() for fact in facts_response.split('\\n') if fact.strip() and '•' in fact or '-' in fact]\n",
    "            \n",
    "            if not facts:\n",
    "                return 0.0\n",
    "            \n",
    "            # Check coverage of each fact in retrieved contexts\n",
    "            covered_facts = 0\n",
    "            contexts_text = \" \".join(retrieved_contexts)\n",
    "            \n",
    "            for fact in facts:\n",
    "                coverage_prompt = f\"\"\"\n",
    "                Does the following context contain information that covers this fact?\n",
    "                \n",
    "                Fact: {fact}\n",
    "                \n",
    "                Context: {contexts_text}\n",
    "                \n",
    "                Answer only 'YES' or 'NO'.\n",
    "                \"\"\"\n",
    "                \n",
    "                coverage_response = self._make_llm_call(coverage_prompt)\n",
    "                if 'YES' in coverage_response.upper():\n",
    "                    covered_facts += 1\n",
    "            \n",
    "            return covered_facts / len(facts) if facts else 0.0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in document recall evaluation: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_context_relevance(self, question: str, retrieved_contexts: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate how relevant the retrieved contexts are to the question.\n",
    "        \"\"\"\n",
    "        if not retrieved_contexts:\n",
    "            return 0.0\n",
    "        \n",
    "        relevant_contexts = 0\n",
    "        \n",
    "        for context in retrieved_contexts:\n",
    "            relevance_prompt = f\"\"\"\n",
    "            Rate the relevance of the following context to the given question on a scale of 0-1.\n",
    "            \n",
    "            Question: {question}\n",
    "            \n",
    "            Context: {context}\n",
    "            \n",
    "            Consider:\n",
    "            - Does the context contain information that could help answer the question?\n",
    "            - Is the context topically related to the question?\n",
    "            - Would this context be useful for generating a response?\n",
    "            \n",
    "            Return only a number between 0 and 1 (e.g., 0.8, 0.3, 1.0).\n",
    "            \"\"\"\n",
    "            \n",
    "            try:\n",
    "                relevance_response = self._make_llm_call(relevance_prompt)\n",
    "                # Extract numeric score\n",
    "                score_match = re.search(r'(\\d+\\.?\\d*)', relevance_response)\n",
    "                if score_match:\n",
    "                    score = float(score_match.group(1))\n",
    "                    # Ensure score is between 0 and 1\n",
    "                    score = min(max(score, 0.0), 1.0)\n",
    "                    relevant_contexts += score\n",
    "                else:\n",
    "                    print(f\"Could not parse relevance score: {relevance_response}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating context relevance: {e}\")\n",
    "        \n",
    "        return relevant_contexts / len(retrieved_contexts) if retrieved_contexts else 0.0\n",
    "    \n",
    "    def evaluate_faithfulness(self, generated_answer: str, retrieved_contexts: List[str]) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate how faithful the generated answer is to the retrieved contexts.\n",
    "        Checks if the answer contains information not supported by the contexts.\n",
    "        \"\"\"\n",
    "        if not retrieved_contexts or not generated_answer:\n",
    "            return 0.0\n",
    "        \n",
    "        # Extract claims from the generated answer\n",
    "        claim_extraction_prompt = f\"\"\"\n",
    "        Extract all factual claims from the following generated answer. \n",
    "        List each claim as a separate bullet point.\n",
    "        \n",
    "        Generated Answer: {generated_answer}\n",
    "        \n",
    "        Factual Claims:\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            claims_response = self._make_llm_call(claim_extraction_prompt)\n",
    "            claims = [claim.strip() for claim in claims_response.split('\\n') \n",
    "                     if claim.strip() and ('•' in claim or '-' in claim)]\n",
    "            \n",
    "            if not claims:\n",
    "                return 1.0  # If no claims extracted, assume faithful\n",
    "            \n",
    "            # Check if each claim is supported by the contexts\n",
    "            supported_claims = 0\n",
    "            contexts_text = \" \".join(retrieved_contexts)\n",
    "            \n",
    "            for claim in claims:\n",
    "                support_prompt = f\"\"\"\n",
    "                Is the following claim supported by or consistent with the provided contexts?\n",
    "                \n",
    "                Claim: {claim}\n",
    "                \n",
    "                Contexts: {contexts_text}\n",
    "                \n",
    "                Answer 'YES' if the claim is supported/consistent, 'NO' if it contradicts or is not supported.\n",
    "                \"\"\"\n",
    "                \n",
    "                support_response = self._make_llm_call(support_prompt)\n",
    "                if 'YES' in support_response.upper():\n",
    "                    supported_claims += 1\n",
    "            \n",
    "            return supported_claims / len(claims) if claims else 1.0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in faithfulness evaluation: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_response_relevance(self, user_input: str, generated_answer: str, num_questions: int = 3) -> float:\n",
    "        \"\"\"\n",
    "        Evaluate response relevancy using the RAGAS methodology:\n",
    "        1. Generate artificial questions based on the response\n",
    "        2. Compute cosine similarity between user input and each generated question\n",
    "        3. Take the average of cosine similarity scores\n",
    "        \"\"\"\n",
    "        if not generated_answer:\n",
    "            return 0.0\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Generate artificial questions based on the response\n",
    "            question_generation_prompt = f\"\"\"\n",
    "            Based on the following response, generate {num_questions} questions that this response would appropriately answer.\n",
    "            The questions should reflect the main content and intent of the response.\n",
    "            \n",
    "            Response: {generated_answer}\n",
    "            \n",
    "            Generate exactly {num_questions} questions, one per line, without numbering or bullets:\n",
    "            \"\"\"\n",
    "            \n",
    "            questions_response = self._make_llm_call(question_generation_prompt)\n",
    "            generated_questions = [q.strip() for q in questions_response.split('\\n') \n",
    "                                 if q.strip() and not q.strip().startswith(('1.', '2.', '3.', '-', '•'))]\n",
    "            \n",
    "            # Clean up questions - remove numbering if present\n",
    "            cleaned_questions = []\n",
    "            for q in generated_questions:\n",
    "                # Remove common prefixes\n",
    "                q = re.sub(r'^\\d+\\.\\s*', '', q)  # Remove \"1. \", \"2. \", etc.\n",
    "                q = re.sub(r'^[-•]\\s*', '', q)   # Remove \"- \" or \"• \"\n",
    "                if q.strip():\n",
    "                    cleaned_questions.append(q.strip())\n",
    "            \n",
    "            if len(cleaned_questions) < num_questions:\n",
    "                # If we didn't get enough questions, pad the list\n",
    "                while len(cleaned_questions) < num_questions:\n",
    "                    cleaned_questions.append(generated_answer[:100] + \"...\")\n",
    "            \n",
    "            # Take only the requested number of questions\n",
    "            generated_questions = cleaned_questions[:num_questions]\n",
    "            \n",
    "            # Step 2: Get embeddings for user input and generated questions\n",
    "            user_embedding = self._get_embedding(user_input)\n",
    "            question_embeddings = [self._get_embedding(q) for q in generated_questions]\n",
    "            \n",
    "            # Step 3: Compute cosine similarities\n",
    "            similarities = []\n",
    "            for q_embedding in question_embeddings:\n",
    "                similarity = self._cosine_similarity(user_embedding, q_embedding)\n",
    "                similarities.append(similarity)\n",
    "            \n",
    "            # Step 4: Return average similarity\n",
    "            return np.mean(similarities) if similarities else 0.0\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in response relevance evaluation: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def _get_embedding(self, text: str) -> List[float]:\n",
    "        \"\"\"Generate embedding for text using Google GenAI\"\"\"\n",
    "        try:\n",
    "            response = google_client.models.embed_content(\n",
    "                model=\"gemini-embedding-001\",\n",
    "                contents=text,\n",
    "                config=types.EmbedContentConfig(\n",
    "                    task_type=\"SEMANTIC_SIMILARITY\",\n",
    "                    output_dimensionality=768,\n",
    "                )\n",
    "            )\n",
    "            return response.embeddings[0].values\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding: {e}\")\n",
    "            # Return zero vector as fallback\n",
    "            return [0.0] * 768\n",
    "    \n",
    "    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:\n",
    "        \"\"\"Calculate cosine similarity between two vectors\"\"\"\n",
    "        try:\n",
    "            vec1 = np.array(vec1)\n",
    "            vec2 = np.array(vec2)\n",
    "            \n",
    "            dot_product = np.dot(vec1, vec2)\n",
    "            norm1 = np.linalg.norm(vec1)\n",
    "            norm2 = np.linalg.norm(vec2)\n",
    "            \n",
    "            if norm1 == 0 or norm2 == 0:\n",
    "                return 0.0\n",
    "            \n",
    "            return dot_product / (norm1 * norm2)\n",
    "        except Exception as e:\n",
    "            print(f\"Error calculating cosine similarity: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def evaluate_single_case(self, \n",
    "                           question: str, \n",
    "                           ground_truth: str,\n",
    "                           retrieved_contexts: List[str], \n",
    "                           generated_answer: str) -> EvaluationResult:\n",
    "        \"\"\"\n",
    "        Evaluate a single RAG case across all metrics\n",
    "        \"\"\"\n",
    "        print(f\"Evaluating question: {question[:100]}...\")\n",
    "        \n",
    "        # Evaluate all metrics\n",
    "        doc_recall = self.evaluate_document_recall(ground_truth, retrieved_contexts)\n",
    "        context_rel = self.evaluate_context_relevance(question, retrieved_contexts)\n",
    "        faithfulness = self.evaluate_faithfulness(generated_answer, retrieved_contexts)\n",
    "        response_rel = self.evaluate_response_relevance(question, generated_answer)\n",
    "        \n",
    "        # Calculate overall score (weighted average)\n",
    "        overall_score = (doc_recall * 0.25 + context_rel * 0.25 + \n",
    "                        faithfulness * 0.25 + response_rel * 0.25)\n",
    "        \n",
    "        return EvaluationResult(\n",
    "            question=question,\n",
    "            ground_truth=ground_truth,\n",
    "            retrieved_contexts=retrieved_contexts,\n",
    "            generated_answer=generated_answer,\n",
    "            document_recall=doc_recall,\n",
    "            context_relevance=context_rel,\n",
    "            faithfulness=faithfulness,\n",
    "            response_relevance=response_rel,\n",
    "            overall_score=overall_score\n",
    "        )\n",
    "    \n",
    "    def evaluate_dataset(self, evaluation_data: List[Dict]) -> Tuple[List[EvaluationResult], Dict]:\n",
    "        \"\"\"\n",
    "        Evaluate an entire dataset of RAG cases\n",
    "        \n",
    "        Args:\n",
    "            evaluation_data: List of dicts with keys:\n",
    "                - 'question': The user question\n",
    "                - 'ground_truth': Expected/ideal answer\n",
    "                - 'retrieved_contexts': List of retrieved context strings\n",
    "                - 'generated_answer': The RAG system's generated answer\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (detailed_results, summary_stats)\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for i, data in enumerate(evaluation_data):\n",
    "            print(f\"\\n--- Evaluating case {i+1}/{len(evaluation_data)} ---\")\n",
    "            \n",
    "            try:\n",
    "                result = self.evaluate_single_case(\n",
    "                    question=data['question'],\n",
    "                    ground_truth=data['ground_truth'],\n",
    "                    retrieved_contexts=data['retrieved_contexts'],\n",
    "                    generated_answer=data['generated_answer']\n",
    "                )\n",
    "                results.append(result)\n",
    "                \n",
    "                print(f\"Document Recall: {result.document_recall:.3f}\")\n",
    "                print(f\"Context Relevance: {result.context_relevance:.3f}\")\n",
    "                print(f\"Faithfulness: {result.faithfulness:.3f}\")\n",
    "                print(f\"Response Relevance: {result.response_relevance:.3f}\")\n",
    "                print(f\"Overall Score: {result.overall_score:.3f}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating case {i+1}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        if results:\n",
    "            summary_stats = {\n",
    "                'total_cases': len(results),\n",
    "                'avg_document_recall': np.mean([r.document_recall for r in results]),\n",
    "                'avg_context_relevance': np.mean([r.context_relevance for r in results]),\n",
    "                'avg_faithfulness': np.mean([r.faithfulness for r in results]),\n",
    "                'avg_response_relevance': np.mean([r.response_relevance for r in results]),\n",
    "                'avg_overall_score': np.mean([r.overall_score for r in results]),\n",
    "                'std_overall_score': np.std([r.overall_score for r in results])\n",
    "            }\n",
    "        else:\n",
    "            summary_stats = {}\n",
    "        \n",
    "        return results, summary_stats\n",
    "    \n",
    "    def export_results(self, results: List[EvaluationResult], filename: str = \"rag_evaluation_results.json\"):\n",
    "        \"\"\"Export evaluation results to JSON file\"\"\"\n",
    "        results_dict = [asdict(result) for result in results]\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(results_dict, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Results exported to {filename}\")\n",
    "    \n",
    "    def create_results_dataframe(self, results: List[EvaluationResult]) -> pd.DataFrame:\n",
    "        \"\"\"Convert results to pandas DataFrame for analysis\"\"\"\n",
    "        df_data = []\n",
    "        \n",
    "        for result in results:\n",
    "            df_data.append({\n",
    "                'question': result.question,\n",
    "                'ground_truth': result.ground_truth,\n",
    "                'generated_answer': result.generated_answer,\n",
    "                'num_contexts': len(result.retrieved_contexts),\n",
    "                'document_recall': result.document_recall,\n",
    "                'context_relevance': result.context_relevance,\n",
    "                'faithfulness': result.faithfulness,\n",
    "                'response_relevance': result.response_relevance,\n",
    "                'overall_score': result.overall_score\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792c24f5",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "Creating the helper functions for generating data and exporting result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a0052bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_evaluation_data(rag_agent, retriever):\n",
    "    \"\"\"\n",
    "    Create sample evaluation data using your actual RAG system.\n",
    "    This generates realistic test cases by running questions through your RAG pipeline.\n",
    "    \n",
    "    Args:\n",
    "        rag_agent: Your RAGAgent instance\n",
    "        retriever: Your ElasticRetriever instance\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test questions and their ground truth answers\n",
    "    test_cases = [\n",
    "        {\n",
    "            \"question\": \"What is the procedure for mouting the motor?\",\n",
    "            \"ground_truth\": \"\"\"MountingFoot mounted machines should be mounted to a rigid foundation to prevent excessive \n",
    "                vibration. Shims may be used if location is uneven.\n",
    "                Flange mounted machines should be properly seated and aligned. Note: If improper \n",
    "                rotation direction is detrimental to the load, check rotation direction prior to coupling the \n",
    "                load to the motor shaft.\n",
    "                For V-belt drive, mount the sheave pulley close to the motor housing. Allow clearance \n",
    "                for end to end movement of the motor shaft. Do not overtighten belts as this may cause \n",
    "                premature bearing failure or shaft breakage.\n",
    "                Direct coupled machines should be carefully aligned and the shaft should rotate freely \n",
    "                without binding.\"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"question\": \"What is the procedure to install Three Phase Motors from Baldor Motors?\",\n",
    "            \"ground_truth\": \"\"\"Installation Procedure\n",
    "\n",
    "                This procedure outlines the steps for the safe and proper installation of a submersible pump motor, focusing on electrical and mechanical precautions and checks.\n",
    "\n",
    "                General Instructions\n",
    "                Prioritize Safety: Turn off and lock out all power before starting. Verify that the voltage at the motor starter connectors is zero.\n",
    "\n",
    "                Handle with Care: Do not use force to drive the pump onto the motor shaft or to remove it, as this can cause damage.\n",
    "\n",
    "                Connection and Verification Procedure\n",
    "                Electrical Connection: Connect the motor power leads to the connectors in the motor starter. The motor's cable assembly has three power leads, two ground leads, two thermal leads, and two moisture sensing probe leads.\n",
    "\n",
    "                Rotation Check (Three-Phase Motors ONLY):\n",
    "\n",
    "                Preparation: Turn off the power, disconnect the motor shaft from the load, and remove any loose rotating parts.\n",
    "\n",
    "                Rotation Test: Momentarily apply power and check the direction of rotation of the motor shaft.\n",
    "\n",
    "                Correcting Rotation: If the shaft rotation is incorrect, turn off the power again and reverse any two of the three motor power leads at the motor starter. Restore power to verify the correct rotation.\n",
    "\n",
    "                Additional Connections: Connect the two Thermal Protectors and the two Moisture Sensing Probes at the motor starter as shown in the manual's diagrams (Figure 2-2 and Figure 2-3).\n",
    "\n",
    "                Assembly and Finalization\n",
    "                Pump Mounting: Follow the pump manufacturer's instructions to mount the pump onto the motor shaft.\n",
    "\n",
    "                Securing: Secure the pump case to the motor flange and attach the drain piping to the pump.\n",
    "\n",
    "                Lowering: Use a spreader bar and lifting eyes to lower the motor/pump assembly to the proper depth. Ensure that the motor wires are not damaged during this process.\n",
    "\n",
    "                Configuration: Set the control parameter values (if applicable) according to the motor nameplate values.\"\"\"\n",
    "        },\n",
    "    ]\n",
    "    \n",
    "    evaluation_data = []\n",
    "    \n",
    "    print(\"Generating evaluation data using your RAG system...\")\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        print(f\"\\nProcessing question {i+1}/{len(test_cases)}: {test_case['question'][:60]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Retrieve contexts using your retriever\n",
    "            retrieved_docs = retriever.retrieve(test_case['question'], top_k=5)\n",
    "            retrieved_contexts = [doc['text'] for doc in retrieved_docs]\n",
    "            \n",
    "            print(f\"  Retrieved {len(retrieved_contexts)} contexts\")\n",
    "            \n",
    "            # Step 2: Generate answer using your RAG agent\n",
    "            rag_response = rag_agent.run(test_case['question'])\n",
    "            \n",
    "            # Extract the generated answer from your RAG response\n",
    "            # Adjust this based on your RAGResponse schema structure\n",
    "            if isinstance(rag_response, dict):\n",
    "                generated_answer = rag_response.get('answer', '') or rag_response.get('response', '') or str(rag_response)\n",
    "            else:\n",
    "                generated_answer = str(rag_response)\n",
    "            \n",
    "            print(f\"  Generated answer: {generated_answer[:100]}...\")\n",
    "            \n",
    "            # Create evaluation data entry\n",
    "            eval_entry = {\n",
    "                'question': test_case['question'],\n",
    "                'ground_truth': test_case['ground_truth'],\n",
    "                'retrieved_contexts': retrieved_contexts,\n",
    "                'generated_answer': generated_answer\n",
    "            }\n",
    "            \n",
    "            evaluation_data.append(eval_entry)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error processing question {i+1}: {e}\")\n",
    "            # Add a fallback entry to keep the evaluation going\n",
    "            evaluation_data.append({\n",
    "                'question': test_case['question'],\n",
    "                'ground_truth': test_case['ground_truth'],\n",
    "                'retrieved_contexts': [\"Error retrieving contexts\"],\n",
    "                'generated_answer': f\"Error generating answer: {str(e)}\"\n",
    "            })\n",
    "    \n",
    "    print(f\"\\nGenerated {len(evaluation_data)} evaluation cases\")\n",
    "    return evaluation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60201f4a",
   "metadata": {},
   "source": [
    "### Run Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34402453",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.pipeline.retrieve import ElasticRetriever\n",
    "from app.pipeline.generate import RAGAgent\n",
    "\n",
    "def run_evaluation_example():\n",
    "    \"\"\"\n",
    "    Example of how to run the RAG evaluation system with sample data\n",
    "    \"\"\"\n",
    "    # Initialize evaluator\n",
    "    evaluator = RAGEvaluator()\n",
    "\n",
    "    retriever = ElasticRetriever(\n",
    "        elastic_url=elastic_url,\n",
    "        api_key=elastic_api_key, \n",
    "        index_name=\"default-evaluation-index\"\n",
    "    )\n",
    "\n",
    "    rag_agent = RAGAgent(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        retriever=retriever\n",
    "    )\n",
    "    \n",
    "    # Create or load your evaluation data\n",
    "    evaluation_data = create_sample_evaluation_data(rag_agent, retriever)\n",
    "    \n",
    "    # Run evaluation\n",
    "    results, summary = evaluator.evaluate_dataset(evaluation_data)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\"*50)\n",
    "    for metric, value in summary.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Create DataFrame for analysis\n",
    "    df = evaluator.create_results_dataframe(results)\n",
    "    \n",
    "    # Export results\n",
    "    evaluator.export_results(results)\n",
    "    \n",
    "    return results, summary, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ea4bc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-08-17 22:56:58] [app.pipeline.retrieve] [retrieve.__init__:40] [INFO] Connected to Elasticsearch at https://my-elasticsearch-project-bfcdc2.es.us-central1.gcp.elastic.cloud:443\n",
      "Generating evaluation data using your RAG system...\n",
      "\n",
      "Processing question 1/2: What is the procedure for mouting the motor?...\n",
      "[2025-08-17 22:56:58] [app.pipeline.retrieve] [retrieve.retrieve:47] [INFO] Running vector search | Top-K: 5 | Query: What is the procedure for mouting the motor?...\n",
      "[2025-08-17 22:57:01] [app.pipeline.retrieve] [retrieve.retrieve:70] [INFO] Retrieved 5 results for query.\n",
      "  Retrieved 5 contexts\n",
      "[2025-08-17 22:57:01] [app.pipeline.generate] [generate.run:53] [INFO] Agent 'RAG Agent is searching for relevant documents'\n",
      "[2025-08-17 22:57:01] [app.pipeline.retrieve] [retrieve.retrieve:47] [INFO] Running vector search | Top-K: 5 | Query: What is the procedure for mouting the motor?...\n",
      "[2025-08-17 22:57:01] [app.pipeline.retrieve] [retrieve.retrieve:70] [INFO] Retrieved 5 results for query.\n",
      "[2025-08-17 22:57:01] [app.pipeline.generate] [generate.run:58] [INFO] Agent 'RAG Agent' found 5 relevant documents from 5 retrieved\n",
      "[2025-08-17 22:57:01] [app.pipeline.generate] [generate.run:124] [INFO] Agent 'RAG Agent' generating response\n",
      "[2025-08-17 22:57:06] [app.pipeline.generate] [generate.run:130] [INFO] Response generated successfully: {\"response\": \"The procedure for mounting a motor v...\n",
      "  Generated answer: The procedure for mounting a motor varies depending on its type (foot-mounted, flange-mounted, or ev...\n",
      "\n",
      "Processing question 2/2: What is the procedure to install Three Phase Motors from Bal...\n",
      "[2025-08-17 22:57:06] [app.pipeline.retrieve] [retrieve.retrieve:47] [INFO] Running vector search | Top-K: 5 | Query: What is the procedure to install Three Phase Motor...\n",
      "[2025-08-17 22:57:07] [app.pipeline.retrieve] [retrieve.retrieve:70] [INFO] Retrieved 5 results for query.\n",
      "  Retrieved 5 contexts\n",
      "[2025-08-17 22:57:07] [app.pipeline.generate] [generate.run:53] [INFO] Agent 'RAG Agent is searching for relevant documents'\n",
      "[2025-08-17 22:57:07] [app.pipeline.retrieve] [retrieve.retrieve:47] [INFO] Running vector search | Top-K: 5 | Query: What is the procedure to install Three Phase Motor...\n",
      "[2025-08-17 22:57:08] [app.pipeline.retrieve] [retrieve.retrieve:70] [INFO] Retrieved 5 results for query.\n",
      "[2025-08-17 22:57:08] [app.pipeline.generate] [generate.run:58] [INFO] Agent 'RAG Agent' found 5 relevant documents from 5 retrieved\n",
      "[2025-08-17 22:57:08] [app.pipeline.generate] [generate.run:124] [INFO] Agent 'RAG Agent' generating response\n",
      "[2025-08-17 22:57:19] [app.pipeline.generate] [generate.run:130] [INFO] Response generated successfully: {\"response\":\"To install a three-phase motor from B...\n",
      "  Generated answer: To install a three-phase motor from Baldor, follow these steps:\n",
      "\n",
      "1.  **Preparation and Safety:**\n",
      "   ...\n",
      "\n",
      "Generated 2 evaluation cases\n",
      "\n",
      "--- Evaluating case 1/2 ---\n",
      "Evaluating question: What is the procedure for mouting the motor?...\n",
      "API call failed (attempt 1/10): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '10'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '15s'}]}}\n",
      "Retrying in 1.05 seconds...\n",
      "API call failed (attempt 2/10): 429 RESOURCE_EXHAUSTED. {'error': {'code': 429, 'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.', 'status': 'RESOURCE_EXHAUSTED', 'details': [{'@type': 'type.googleapis.com/google.rpc.QuotaFailure', 'violations': [{'quotaMetric': 'generativelanguage.googleapis.com/generate_content_free_tier_requests', 'quotaId': 'GenerateRequestsPerMinutePerProjectPerModel-FreeTier', 'quotaDimensions': {'location': 'global', 'model': 'gemini-2.5-flash'}, 'quotaValue': '10'}]}, {'@type': 'type.googleapis.com/google.rpc.Help', 'links': [{'description': 'Learn more about Gemini API quotas', 'url': 'https://ai.google.dev/gemini-api/docs/rate-limits'}]}, {'@type': 'type.googleapis.com/google.rpc.RetryInfo', 'retryDelay': '14s'}]}}\n",
      "Retrying in 2.53 seconds...\n",
      "API call failed (attempt 1/10): 'NoneType' object has no attribute 'strip'\n",
      "Retrying in 1.04 seconds...\n",
      "API call failed (attempt 1/10): 'NoneType' object has no attribute 'strip'\n",
      "Retrying in 1.66 seconds...\n",
      "Document Recall: 1.000\n",
      "Context Relevance: 0.540\n",
      "Faithfulness: 1.000\n",
      "Response Relevance: 0.893\n",
      "Overall Score: 0.858\n",
      "\n",
      "--- Evaluating case 2/2 ---\n",
      "Evaluating question: What is the procedure to install Three Phase Motors from Baldor Motors?...\n",
      "API call failed (attempt 1/10): 'NoneType' object has no attribute 'strip'\n",
      "Retrying in 1.71 seconds...\n",
      "Document Recall: 1.000\n",
      "Context Relevance: 0.880\n",
      "Faithfulness: 1.000\n",
      "Response Relevance: 0.920\n",
      "Overall Score: 0.950\n",
      "\n",
      "==================================================\n",
      "EVALUATION SUMMARY\n",
      "==================================================\n",
      "total_cases: 2.0000\n",
      "avg_document_recall: 1.0000\n",
      "avg_context_relevance: 0.7100\n",
      "avg_faithfulness: 1.0000\n",
      "avg_response_relevance: 0.9065\n",
      "avg_overall_score: 0.9041\n",
      "std_overall_score: 0.0460\n",
      "Results exported to rag_evaluation_results.json\n",
      "                                            question  \\\n",
      "0       What is the procedure for mouting the motor?   \n",
      "1  What is the procedure to install Three Phase M...   \n",
      "\n",
      "                                        ground_truth  \\\n",
      "0  MountingFoot mounted machines should be mounte...   \n",
      "1  Installation Procedure\\n\\n                This...   \n",
      "\n",
      "                                    generated_answer  num_contexts  \\\n",
      "0  The procedure for mounting a motor varies depe...             5   \n",
      "1  To install a three-phase motor from Baldor, fo...             5   \n",
      "\n",
      "   document_recall  context_relevance  faithfulness  response_relevance  \\\n",
      "0              1.0               0.54           1.0            0.892588   \n",
      "1              1.0               0.88           1.0            0.920481   \n",
      "\n",
      "   overall_score  \n",
      "0       0.858147  \n",
      "1       0.950120  \n"
     ]
    }
   ],
   "source": [
    "results, summary, df = run_evaluation_example()\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "industrial-rag-pipeline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
